GGML_CUDA_NO_PINNED=1
IMAGE_TAG=llama-cpp-docker
LLAMA_HOST="0.0.0.0"
LLAMA_PORT="8080"
OUTER_PORT="8080"
LLAMA_CTX_SIZE=2048
LLAMA_MODEL=/models/llama-2-13b-chat.Q5_K_M.gguf

